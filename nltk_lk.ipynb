{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yedqsxyHAG3W",
        "outputId": "cedc0d40-b11c-49bc-f586-8109adff7000"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming: ['run', 'run', 'easili', 'fair', 'fair']\n",
            "Lemmatization (NLTK): ['running', 'run', 'easily', 'fair', 'fairness']\n",
            "Lemmatization (spaCy): ['run', 'run', 'easily', 'fair', 'fairness']\n",
            "POS Tagging: [('running', 'VBG'), ('runs', 'NNS'), ('easily', 'RB'), ('fair', 'JJ'), ('fairness', 'NN')]\n",
            "running -> amod -> runs\n",
            "runs -> ROOT -> runs\n",
            "easily -> advmod -> fair\n",
            "fair -> amod -> fairness\n",
            "fairness -> dobj -> runs\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Sample text\n",
        "text = \"running runs easily fair fairness\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "print(\"Stemming:\", stemmed_words)\n",
        "\n",
        "# Lemmatization using NLTK\n",
        "lemmatized_words_nltk = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "print(\"Lemmatization (NLTK):\", lemmatized_words_nltk)\n",
        "\n",
        "# Lemmatization using spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "lemmatized_words_spacy = [token.lemma_ for token in doc]\n",
        "print(\"Lemmatization (spaCy):\", lemmatized_words_spacy)\n",
        "\n",
        "# Part-of-Speech (POS) Tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(\"POS Tagging:\", pos_tags)\n",
        "\n",
        "# Dependency Parsing\n",
        "for token in doc:\n",
        "    print(f\"{token.text} -> {token.dep_} -> {token.head.text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install allennlp allennlp-models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6NdfWwcBSW6",
        "outputId": "63a792c3-2389-42ba-e143-6b99318fd611"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting allennlp\n",
            "  Downloading allennlp-2.10.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting allennlp-models\n",
            "  Downloading allennlp_models-2.10.1-py3-none-any.whl.metadata (23 kB)\n",
            "INFO: pip is looking at multiple versions of allennlp to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting allennlp\n",
            "  Downloading allennlp-2.10.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading allennlp-2.9.3-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading allennlp-2.9.2-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading allennlp-2.9.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading allennlp-2.9.0-py3-none-any.whl.metadata (18 kB)\n",
            "  Downloading allennlp-2.8.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading allennlp-2.7.0-py3-none-any.whl.metadata (17 kB)\n",
            "INFO: pip is still looking at multiple versions of allennlp to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading allennlp-2.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading allennlp-2.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading allennlp-2.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading allennlp-2.3.1-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading allennlp-2.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading allennlp-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading allennlp-2.1.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading allennlp-2.0.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading allennlp-2.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading allennlp-1.5.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading allennlp-1.4.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading allennlp-1.4.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading allennlp-1.3.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading allennlp-1.2.2-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading allennlp-1.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "  Downloading allennlp-1.2.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading allennlp-1.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading allennlp-1.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading allennlp-0.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from allennlp) (2.5.1+cu124)\n",
            "Collecting overrides (from allennlp)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from allennlp) (3.9.1)\n",
            "Collecting spacy<2.2,>=2.1.0 (from allennlp)\n",
            "  Downloading spacy-2.1.9.tar.gz (30.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "from nltk.wsd import lesk\n",
        "from collections import defaultdict\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Sample text\n",
        "text = \"John met Mary in New York. He said he would visit her again next Monday.\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "print(\"Stemming:\", stemmed_words)\n",
        "\n",
        "# Lemmatization using NLTK\n",
        "lemmatized_words_nltk = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "print(\"Lemmatization (NLTK):\", lemmatized_words_nltk)\n",
        "\n",
        "# Lemmatization using spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "lemmatized_words_spacy = [token.lemma_ for token in doc]\n",
        "print(\"Lemmatization (spaCy):\", lemmatized_words_spacy)\n",
        "\n",
        "# Part-of-Speech (POS) Tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(\"POS Tagging:\", pos_tags)\n",
        "\n",
        "# Dependency Parsing\n",
        "for token in doc:\n",
        "    print(f\"{token.text} -> {token.dep_} -> {token.head.text}\")\n",
        "\n",
        "\n",
        "# Named Entity Recognition (NER)\n",
        "print(\"Named Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} ({ent.label_})\")\n",
        "\n",
        "# Word Sense Disambiguation (WSD)\n",
        "wsd_results = {word: lesk(tokens, word) for word in tokens}\n",
        "print(\"Word Sense Disambiguation:\", {word: synset.definition() if synset else None for word, synset in wsd_results.items()})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiGYK_5DAKqf",
        "outputId": "832dc2fc-1593-4c06-8d49-28d5b233d019"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming: ['john', 'met', 'mari', 'in', 'new', 'york', '.', 'he', 'said', 'he', 'would', 'visit', 'her', 'again', 'next', 'monday', '.']\n",
            "Lemmatization (NLTK): ['John', 'met', 'Mary', 'in', 'New', 'York', '.', 'He', 'said', 'he', 'would', 'visit', 'her', 'again', 'next', 'Monday', '.']\n",
            "Lemmatization (spaCy): ['John', 'meet', 'Mary', 'in', 'New', 'York', '.', 'he', 'say', 'he', 'would', 'visit', 'she', 'again', 'next', 'Monday', '.']\n",
            "POS Tagging: [('John', 'NNP'), ('met', 'VBD'), ('Mary', 'NNP'), ('in', 'IN'), ('New', 'NNP'), ('York', 'NNP'), ('.', '.'), ('He', 'PRP'), ('said', 'VBD'), ('he', 'PRP'), ('would', 'MD'), ('visit', 'VB'), ('her', 'PRP$'), ('again', 'RB'), ('next', 'JJ'), ('Monday', 'NNP'), ('.', '.')]\n",
            "John -> nsubj -> met\n",
            "met -> ROOT -> met\n",
            "Mary -> dobj -> met\n",
            "in -> prep -> met\n",
            "New -> compound -> York\n",
            "York -> pobj -> in\n",
            ". -> punct -> met\n",
            "He -> nsubj -> said\n",
            "said -> ROOT -> said\n",
            "he -> nsubj -> visit\n",
            "would -> aux -> visit\n",
            "visit -> ccomp -> said\n",
            "her -> dobj -> visit\n",
            "again -> advmod -> visit\n",
            "next -> amod -> Monday\n",
            "Monday -> npadvmod -> visit\n",
            ". -> punct -> said\n",
            "Named Entities:\n",
            "John (PERSON)\n",
            "Mary (PERSON)\n",
            "New York (GPE)\n",
            "next Monday (DATE)\n",
            "Word Sense Disambiguation: {'John': 'the last of the four Gospels in the New Testament', 'met': 'be in direct physical contact with; make contact', 'Mary': 'the mother of Jesus; Christians refer to her as the Virgin Mary; she is especially honored by Roman Catholics', 'in': 'a rare soft silvery metallic element; occurs in small quantities in sphalerite', 'New': 'in use after medieval times', 'York': 'the English royal house (a branch of the Plantagenet line) that reigned from 1461 to 1485; its emblem was a white rose', '.': None, 'He': 'a very light colorless element that is one of the six inert gasses; the most difficult gas to liquefy; occurs in economically extractable amounts in certain natural gases (as those found in Texas and Kansas)', 'said': 'express in words', 'he': 'a very light colorless element that is one of the six inert gasses; the most difficult gas to liquefy; occurs in economically extractable amounts in certain natural gases (as those found in Texas and Kansas)', 'would': None, 'visit': 'come to see in an official or professional capacity', 'her': None, 'again': 'anew', 'next': 'immediately following in time or order', 'Monday': 'the second day of the week; the first working day'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Span\n",
        "from spacy.pipeline import EntityRuler\n",
        "\n",
        "def extract_entities_relations(text):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    relations = []\n",
        "\n",
        "    for token in doc:\n",
        "        if token.dep_ in (\"nsubj\", \"dobj\", \"pobj\") and token.head.pos_ in (\"VERB\", \"NOUN\"):\n",
        "            relations.append((token.text, token.dep_, token.head.text))\n",
        "\n",
        "    return entities, relations\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    text = \"I just met a shop keeper in Karachi.\"\n",
        "    entities, relations = extract_entities_relations(text)\n",
        "\n",
        "    print(\"Entities:\")\n",
        "    for entity in entities:\n",
        "        print(entity)\n",
        "\n",
        "    print(\"\\nRelations:\")\n",
        "    for relation in relations:\n",
        "        print(relation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7pkEiOwBPNd",
        "outputId": "eb669eea-4306-4ddb-d584-20d31ced0e83"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities:\n",
            "('Karachi', 'GPE')\n",
            "\n",
            "Relations:\n",
            "('I', 'nsubj', 'met')\n",
            "('keeper', 'dobj', 'met')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample dataset for demonstration\n",
        "data = pd.DataFrame({\n",
        "    'text': [\n",
        "        \"I love this product! It's amazing.\",\n",
        "        \"This is the worst experience ever.\",\n",
        "        \"I feel okay about this.\",\n",
        "        \"Exclusive deal just for you! Buy now!\",\n",
        "        \"Hello, hope you are doing well!\",\n",
        "        \"Congratulations! You won a lottery. Click here!\"\n",
        "    ],\n",
        "    'sentiment': ['positive', 'negative', 'neutral', 'spam', 'not spam', 'spam']\n",
        "})\n",
        "\n",
        "# Sentiment Analysis\n",
        "def sentiment_analysis():\n",
        "    df = data[data['sentiment'].isin(['positive', 'negative', 'neutral'])]\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(df['text'])\n",
        "    y = df['sentiment']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model = MultinomialNB()\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    print(\"Sentiment Analysis Accuracy:\", accuracy_score(y_test, predictions))\n",
        "\n",
        "# Topic Modeling\n",
        "def topic_modeling():\n",
        "    vectorizer = CountVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(data['text'])\n",
        "    lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
        "    lda.fit(X)\n",
        "    print(\"Topic Modeling (LDA) Results:\")\n",
        "    for index, topic in enumerate(lda.components_):\n",
        "        print(f\"Topic {index}:\", [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-5:]])\n",
        "\n",
        "# Spam Detection\n",
        "def spam_detection():\n",
        "    df = data[data['sentiment'].isin(['spam', 'not spam'])]\n",
        "    df['label'] = df['sentiment'].apply(lambda x: 1 if x == 'spam' else 0)\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(df['text'])\n",
        "    y = df['label']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model = MultinomialNB()\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    print(\"Spam Detection Accuracy:\", accuracy_score(y_test, predictions))\n",
        "\n",
        "# Running the functions\n",
        "spam_detection()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiNgMvh-CUdk",
        "outputId": "387cb54a-0712-4f1f-d883-cd2291b4d78f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spam Detection Accuracy: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "<ipython-input-17-b6fa7a9732ca>:54: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['label'] = df['sentiment'].apply(lambda x: 1 if x == 'spam' else 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7mIgYoPwEDiW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "overall-april",
   "metadata": {},
   "source": [
    "# Practicing regular expressions: re.split() and re.findall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bearing-danish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The sharp fox jumped over the lazy dog']\n",
      "['The']\n",
      "['The', 'sharp', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "my_string = 'The sharp fox jumped over the lazy dog'\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-insight",
   "metadata": {},
   "source": [
    "# Introduction to tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "helpful-azerbaijan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'bro', '!']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk library \n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize('Hello bro!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-development",
   "metadata": {},
   "source": [
    "# Regex Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "progressive-legislature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match('abc', 'abcde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bound-narrow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('abc', 'abcde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coupled-acoustic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(2, 4), match='cd'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match('cd', 'abcde')\n",
    "re.search('cd', 'abcde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "consolidated-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "##match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "##print(match.start(), match.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sized-electricity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "##pattern1 = r\"\\[.*\\]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "##print(re.search(pattern1, scene_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "every-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "##sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "##tokenized_sent = word_tokenize(sentences)\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "##unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "##print(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intellectual-hunger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "##pattern2 = r\"[\\w\\s]+:\"\n",
    "##print(re.match(pattern2, sentences[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "likely-democracy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'has', '11', 'cats']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "match_digits_and_words = ('(\\d+|\\w+)')\n",
    "re.findall(match_digits_and_words, 'He has 11 cats')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-gossip",
   "metadata": {},
   "source": [
    "# Regex Ranges and Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-rachel",
   "metadata": {},
   "source": [
    "# [A-Za-z]+ -> 'ABCDEFghijk'(example)\n",
    "# [0-9] -> 9 (example)\n",
    "# [A-Za-z\\-\\.]+ -> 'My.Website.com' (example)\n",
    "# (\\s+|) -> ',' (example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-insight",
   "metadata": {},
   "source": [
    "# Advanced Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "substantial-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "##pattern1 = r\"#\\w+\"\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "##hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "##print(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "helpful-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Write a pattern that matches both mentions (@) and hashtags\n",
    "## pattern2 = r\"([@#]\\w+)\"\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "##mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
    "##print(mentions_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rising-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "##tknzr = TweetTokenizer()\n",
    "##all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "##print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-passenger",
   "metadata": {},
   "source": [
    "# Non-ascii tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cardiac-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and print all words in german_text\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize\n",
    "##all_words = word_tokenize(german_text)\n",
    "##print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "##capital_words = r\"[A-ZÜ]\\w+\"\n",
    "##print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "##emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "##print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-sunset",
   "metadata": {},
   "source": [
    "# Charting word length with NLTK\n",
    "# Combining NLP data extraction with plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "amino-accordance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 0., 0., 0., 2., 0., 0., 0., 1.]),\n",
       " array([2. , 2.4, 2.8, 3.2, 3.6, 4. , 4.4, 4.8, 5.2, 5.6, 6. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize('This is pretty cool')\n",
    "words_length = [len(w) for w in words]\n",
    "plt.hist(words_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "finite-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the script into lines: lines\n",
    "##lines = holy_grail.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "##pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "##lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "##tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "##line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "##plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "##plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-spectrum",
   "metadata": {},
   "source": [
    "# Bag of Words in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "functioning-drink",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'The': 3,\n",
       "         'cat': 3,\n",
       "         'is': 2,\n",
       "         'in': 1,\n",
       "         'the': 3,\n",
       "         'box': 3,\n",
       "         '..': 1,\n",
       "         'Likes': 1,\n",
       "         '.': 2,\n",
       "         'over': 1})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "Counter(word_tokenize(\"\"\"The cat is in the box.. The cat Likes the box. The box is over the cat.\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "attached-dividend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 3), ('cat', 3)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter(word_tokenize(\"\"\"The cat is in the box.. The cat Likes the box. The box is over the cat.\"\"\"))\n",
    "counter.most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-peeing",
   "metadata": {},
   "source": [
    "# Tokenize the Article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "italic-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "##tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "##lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "##bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "##print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-member",
   "metadata": {},
   "source": [
    "# Text preprocessing with python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "descending-lincoln",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\muham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "romance-warrant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 3), ('box', 3)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords \n",
    "text = \"\"\"The cat is in the box.. The cat Likes the box. The box is over the cat.\"\"\"\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "tokens = [w for w in word_tokenize(text.lower())\n",
    "                          if w.isalpha()]\n",
    "\n",
    "no_stops = [t for t in tokens\n",
    "                       if t not in stopwords.words('english')]\n",
    "\n",
    "Counter(no_stops).most_common(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "useful-breach",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\muham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "grand-republic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 6), ('is', 2), ('in', 1), ('over', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in no_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-experience",
   "metadata": {},
   "source": [
    "# Genism - Topic modelling for humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "worldwide-blake",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muham\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary \n",
    "from nltk.tokenize import word_tokenize\n",
    "my_documents = ['I love this movie', ' Shes a killer', ' We can rule the world']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "regulation-bread",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 0,\n",
       " 'love': 1,\n",
       " 'movie': 2,\n",
       " 'this': 3,\n",
       " 'a': 4,\n",
       " 'killer': 5,\n",
       " 'shes': 6,\n",
       " 'can': 7,\n",
       " 'rule': 8,\n",
       " 'the': 9,\n",
       " 'we': 10,\n",
       " 'world': 11}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "forbidden-egyptian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
       " [(4, 1), (5, 1), (6, 1)],\n",
       " [(7, 1), (8, 1), (9, 1), (10, 1), (11, 1)]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus  =[dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "helpful-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "##dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "##computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "##print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "##corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "##print(corpus[4][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "amino-carol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 1\n",
      "killer 1\n",
      "shes 1\n"
     ]
    }
   ],
   "source": [
    "# Save the fifth document: doc\n",
    "doc = corpus[1]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "##total_word_count = deafult_dict(int)\n",
    "##for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    ##total_word_count[word_id] += word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-collection",
   "metadata": {},
   "source": [
    "# Tf-idf with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "spread-alexander",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 0.5773502691896258), (5, 0.5773502691896258), (6, 0.5773502691896258)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "tfidf = TfidfModel(corpus)\n",
    "tfidf[corpus[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "disturbed-nature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 0.5773502691896258), (5, 0.5773502691896258), (6, 0.5773502691896258)]\n"
     ]
    }
   ],
   "source": [
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "moved-innocent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 0.5773502691896258), (5, 0.5773502691896258), (6, 0.5773502691896258)]\n",
      "a 0.5773502691896258\n",
      "killer 0.5773502691896258\n",
      "shes 0.5773502691896258\n"
     ]
    }
   ],
   "source": [
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-visitor",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "legitimate-meditation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\muham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecological-significance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'), ('cat', 'NN'), ('is', 'VBZ')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = \"\"\"The cat is in the box.. The cat Likes the box. The box is over the cat.\"\"\"\n",
    "tokenize_sent = nltk.word_tokenize(sentence)\n",
    "tagged_sent =nltk.pos_tag(tokenize_sent)\n",
    "tagged_sent[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "industrial-filename",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\muham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "constant-quality",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\muham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "motivated-packet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  cat/NN\n",
      "  is/VBZ\n",
      "  in/IN\n",
      "  the/DT\n",
      "  box/NN\n",
      "  ../VBZ\n",
      "  The/DT\n",
      "  cat/NN\n",
      "  Likes/VBZ\n",
      "  the/DT\n",
      "  box/NN\n",
      "  ./.\n",
      "  The/DT\n",
      "  box/NN\n",
      "  is/VBZ\n",
      "  over/IN\n",
      "  the/DT\n",
      "  cat/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.ne_chunk(tagged_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "abandoned-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the defaultdict: ner_categories\n",
    "## ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "##for sent in chunked_sentences:\n",
    "    ##for chunk in sent:\n",
    "        ##if hasattr(chunk, 'label'):\n",
    "            ##ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "##labels = list(ner_categories.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "smaller-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the defaultdict: ner_categories\n",
    "##ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "##for sent in chunked_sentences:\n",
    "    ##for chunk in sent:\n",
    "        ##if hasattr(chunk, 'label'):\n",
    "            ##ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "##labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "##values = [ner_categories.get(v) for v in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "##plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "##plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-timing",
   "metadata": {},
   "source": [
    "# Introduction to SpaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cutting-america",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacyNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading spacy-3.0.6-cp38-cp38-win_amd64.whl (11.9 MB)\n",
      "Collecting catalogue<2.1.0,>=2.0.3\n",
      "  Downloading catalogue-2.0.3-py3-none-any.whl (16 kB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\muham\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\muham\\anaconda3\\lib\\site-packages (from spacy) (20.9)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.4\n",
      "  Downloading spacy_legacy-3.0.5-py2.py3-none-any.whl (12 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.5.2-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\muham\\anaconda3\\lib\\site-packages (from spacy) (1.19.2)\n",
      "Collecting typer<0.4.0,>=0.3.0\n",
      "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.4-cp38-cp38-win_amd64.whl (6.5 MB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\muham\\anaconda3\\lib\\site-packages (from spacy) (4.56.0)\n",
      "Collecting pydantic<1.8.0,>=1.7.1\n",
      "  Downloading pydantic-1.7.3-cp38-cp38-win_amd64.whl (1.8 MB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\muham\\anaconda3\\lib\\site-packages (from spacy) (52.0.0.post20210125)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.5-cp38-cp38-win_amd64.whl (112 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\muham\\anaconda3\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Collecting thinc<8.1.0,>=8.0.3\n",
      "  Downloading thinc-8.0.3-cp38-cp38-win_amd64.whl (1.0 MB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.5-cp38-cp38-win_amd64.whl (36 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.5-cp38-cp38-win_amd64.whl (21 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.1-cp38-cp38-win_amd64.whl (451 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\muham\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Collecting smart-open<4.0.0,>=2.2.0\n",
      "  Downloading smart_open-3.0.0.tar.gz (113 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\muham\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\muham\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\muham\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\muham\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\muham\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\muham\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107097 sha256=9df1f5f01e5674a010a8fd29090586502751b4595757d8fc9038d32acaf4899c\n",
      "  Stored in directory: c:\\users\\muham\\appdata\\local\\pip\\cache\\wheels\\11\\73\\9a\\f91ac1f1816436b16423617c5be5db048697ff152a9c4346f2\n",
      "Successfully built smart-open\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, smart-open, pydantic, preshed, blis, thinc, spacy-legacy, pathy, spacy\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 5.0.0\n",
      "    Uninstalling smart-open-5.0.0:\n",
      "      Successfully uninstalled smart-open-5.0.0\n",
      "Successfully installed blis-0.7.4 catalogue-2.0.3 cymem-2.0.5 murmurhash-1.0.5 pathy-0.5.2 preshed-3.0.5 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.6 spacy-legacy-3.0.5 srsly-2.4.1 thinc-8.0.3 typer-0.3.2 wasabi-0.8.2\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "billion-element",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'English' object has no attribute 'entity'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-f32578c79246>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'English' object has no attribute 'entity'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "extended-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy can be used to create an easy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "modern-villa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "##nlp = spacy.load('en', tagger=False, parser=False, matcher=False)\n",
    "\n",
    "# Create a new document: doc\n",
    "##doc = nlp(article)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "##for ent in doc.ents:\n",
    "    ##print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-boutique",
   "metadata": {},
   "source": [
    "# Multilingual NER with polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "unavailable-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new text object using Polyglot's Text class: txt\n",
    "##txt = Text(article)\n",
    "\n",
    "# Print each of the entities found\n",
    "##for ent in txt.entities:\n",
    "    ##print(ent)\n",
    "    \n",
    "# Print the type of ent\n",
    "##print(type(ent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "pharmaceutical-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of tuples: entities\n",
    "##entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
    "\n",
    "# Print entities\n",
    "##print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "rental-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the count variable: count\n",
    "##count = 0\n",
    "\n",
    "# Iterate over all the entities\n",
    "##for ent in txt.entities:\n",
    "    # Check whether the entity contains 'Márquez' or 'Gabo'\n",
    "    ##if \"Márquez\" in ent or \"Gabo\" in ent:\n",
    "        # Increment count\n",
    "        ##count += 1\n",
    "\n",
    "# Print count\n",
    "##print(count)\n",
    "\n",
    "# Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
    "##percentage = count / len(txt.entities)\n",
    "##print(percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-counter",
   "metadata": {},
   "source": [
    "# Building word count vectors with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "offensive-slovenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "## df = file path\n",
    "## tx, tex, ty, tey = train_test_split(df['plot'], y, test_size=0.33, random_state=55)\n",
    "## count_vectorizer = CountVectorizer(stop_words='english')\n",
    "## count_train = count_vectorizer.fit_transform(tx.values)\n",
    "## count_test = count_vectorizer.transform(tex.values)\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "##print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-reynolds",
   "metadata": {},
   "source": [
    "# TfidfVectorizer for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "instructional-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "##from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "##tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "##tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "##tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "##print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "##print(tfidf_train.A[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-recycling",
   "metadata": {},
   "source": [
    "# Inspecting the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "herbal-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "##count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "##tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "##print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "##print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "##difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "##print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "##print(count_df.equals(tfidf_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-health",
   "metadata": {},
   "source": [
    "# Training and testing a classification model with scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "appreciated-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes with Sckit-learn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "nb_classifier = MultinomialNB()\n",
    "## nb.classifier.fit(count_train, train_y)\n",
    "## predict = nb.classifier.predict(count_test)\n",
    "## metrics.accuracy_score(y_test, pred)\n",
    "## metrics.confusion_matrix(y_test, predict, labels[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "exterior-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of alphas: alphas\n",
    "##alphas = np.arange(0, 1, .1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "##def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "   ## nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "   ## nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "   ## pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    ##score = metrics.accuracy_score(y_test, pred)\n",
    "    ##return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "##for alpha in alphas:\n",
    "   ## print('Alpha: ', alpha)\n",
    "   ## print('Score: ', train_and_predict(alpha))\n",
    "   ## print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "satisfied-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect your model\n",
    "# Get the class labels: class_labels\n",
    "##class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "##feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "##feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "##print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "##print(class_labels[1], feat_with_weights[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-zoning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
